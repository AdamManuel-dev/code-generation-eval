Hereâ€™s a README.md for the Local AI Code Quality Evaluation System, structured for clarity, usability, and professional presentation:

â¸»


# ğŸ§ª Local AI Code Quality Evaluation System

A local, deterministic tool for evaluating the quality, correctness, and robustness of Node.js/TypeScript codebasesâ€”especially those generated by AI. This system performs static and dynamic analysis entirely offline, optionally augmented by LLM-based reviews, providing fast and actionable insights to help developers maintain high code standards.

---

## âœ¨ Features

- ğŸ” **Strict TypeScript Compilation** â€“ Validates code against the most rigorous type-checking rules.
- ğŸ¯ **ESLint + Prettier Enforcement** â€“ Ensures style and static correctness using pluggable linting tools.
- ğŸ§ª **Automated Test Execution & Coverage Analysis** â€“ Measures test reliability and code coverage.
- ğŸ“ **API Schema Conformance** â€“ Validates outputs against Zod or OpenAPI schemas.
- âš™ï¸ **Runtime Behavior Checks** â€“ Tests watcher responsiveness and simulates race conditions.
- ğŸ§  **LLM-Based Code Review (Optional)** â€“ Uses GPT or local LLMs to provide qualitative feedback on structure and maintainability.
- ğŸ“Š **Aggregated Scoring Report** â€“ Outputs JSON and console summaries with granular breakdowns.
- ğŸ”§ **Customizable Thresholds & Checks** â€“ Configure quality gates per project needs.

---

## ğŸš€ Quick Start

### 1. Install

```bash
npm install -g aiq-quality-tester

Or run via npx:

npx aiq-quality-tester ./my-project

2. Run Evaluation

aiqtest ./my-project --output report.json --enable-llm


â¸»

ğŸ› ï¸ CLI Options

Option	Description
--output <file>	Write JSON report to specified file (default: quality-report.json)
`â€“format json	text`
--categories compile,lint,tests	Run only selected checks
--skip schema,llm	Skip selected checks
--enable-llm	Run optional LLM review
--openai-api-key <key>	Provide key for OpenAI model (if using LLM review)
--coverage-threshold <number>	Override default 80% test coverage target
--schema <path>	Specify OpenAPI schema path
--verbose	Show detailed console output
--strict-exit-code	Exit with code 1 on any quality failure


â¸»

ğŸ“ JSON Report Structure (Example)

{
  "project": "my-project",
  "overall": {
    "passed": false,
    "score": 48,
    "maxScore": 50,
    "percentage": 96,
    "criticalFailures": ["linting"]
  },
  "categories": [
    {
      "name": "Compilation",
      "passed": true,
      "score": 10,
      "details": { "errors": 0 }
    },
    {
      "name": "Linting",
      "passed": false,
      "score": 8,
      "details": {
        "eslintErrors": 2,
        "prettierIssues": 5
      }
    },
    ...
  ]
}


â¸»

ğŸ”„ Scoring Overview

Category	Weight	Criteria
Compilation	10	Must compile in strict mode
Linting & Formatting	10	Zero ESLint errors + Prettier passes
Testing & Coverage	10	All tests pass + coverage above threshold
Schema Validation	10	All API/schema outputs match expectations
Runtime Behavior	10	Watchers and concurrency checks pass
LLM Review (Optional)	â€”	Advisory only unless enabled in scoring


â¸»

ğŸ§© Extensibility
	â€¢	Add new check categories via modular plugin interface.
	â€¢	Supports project-specific config via aiqtest.config.json.
	â€¢	Designed for future support of other ecosystems (Python, Java, etc).

â¸»

ğŸ§ª Example Output (Console Summary)

Compilation: PASS (10/10)
Linting: FAIL (8/10) â€“ 2 ESLint errors, 5 files need formatting
Tests: PASS (10/10) â€“ 120 tests passed, coverage 82%
Schema: PASS (10/10)
Runtime: PASS (10/10)
LLM Review: (skipped)

Overall: 48/50 (96%) â€“ QUALITY OK


â¸»

ğŸ” Security & Privacy
	â€¢	No code is ever sent to external services by default.
	â€¢	LLM features require explicit opt-in and API key config.
	â€¢	Runtime checks run in subprocesses with sandboxing where feasible.

â¸»

ğŸ“ˆ Roadmap Highlights
	â€¢	Security vulnerability scanning
	â€¢	Support for Python, Java, etc.
	â€¢	HTML/Markdown report generation
	â€¢	GitHub Action / CI Mode
	â€¢	VS Code Extension

â¸»

ğŸ¤ Contributing

We welcome contributions for custom checks, rule improvements, or language extensions. Future versions will include a plugin API and configuration schema.

â¸»

ğŸ“„ License

MIT

â¸»

ğŸ’¬ Questions?

Open an issue or discussion on the GitHub repo or reach out via email.

Let me know if you'd like this rewritten for publishing to npm (with `bin` section, `keywords`, etc.) or as a GitHub landing page with screenshots and badges.
