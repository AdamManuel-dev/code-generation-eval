Here’s a README.md for the Local AI Code Quality Evaluation System, structured for clarity, usability, and professional presentation:

⸻


# 🧪 Local AI Code Quality Evaluation System

A local, deterministic tool for evaluating the quality, correctness, and robustness of Node.js/TypeScript codebases—especially those generated by AI. This system performs static and dynamic analysis entirely offline, optionally augmented by LLM-based reviews, providing fast and actionable insights to help developers maintain high code standards.

---

## ✨ Features

- 🔍 **Strict TypeScript Compilation** – Validates code against the most rigorous type-checking rules.
- 🎯 **ESLint + Prettier Enforcement** – Ensures style and static correctness using pluggable linting tools.
- 🧪 **Automated Test Execution & Coverage Analysis** – Measures test reliability and code coverage.
- 📐 **API Schema Conformance** – Validates outputs against Zod or OpenAPI schemas.
- ⚙️ **Runtime Behavior Checks** – Tests watcher responsiveness and simulates race conditions.
- 🧠 **LLM-Based Code Review (Optional)** – Uses GPT or local LLMs to provide qualitative feedback on structure and maintainability.
- 📊 **Aggregated Scoring Report** – Outputs JSON and console summaries with granular breakdowns.
- 🔧 **Customizable Thresholds & Checks** – Configure quality gates per project needs.

---

## 🚀 Quick Start

### 1. Install

```bash
npm install -g aiq-quality-tester

Or run via npx:

npx aiq-quality-tester ./my-project

2. Run Evaluation

aiqtest ./my-project --output report.json --enable-llm


⸻

🛠️ CLI Options

Option	Description
--output <file>	Write JSON report to specified file (default: quality-report.json)
`–format json	text`
--categories compile,lint,tests	Run only selected checks
--skip schema,llm	Skip selected checks
--enable-llm	Run optional LLM review
--openai-api-key <key>	Provide key for OpenAI model (if using LLM review)
--coverage-threshold <number>	Override default 80% test coverage target
--schema <path>	Specify OpenAPI schema path
--verbose	Show detailed console output
--strict-exit-code	Exit with code 1 on any quality failure


⸻

📁 JSON Report Structure (Example)

{
  "project": "my-project",
  "overall": {
    "passed": false,
    "score": 48,
    "maxScore": 50,
    "percentage": 96,
    "criticalFailures": ["linting"]
  },
  "categories": [
    {
      "name": "Compilation",
      "passed": true,
      "score": 10,
      "details": { "errors": 0 }
    },
    {
      "name": "Linting",
      "passed": false,
      "score": 8,
      "details": {
        "eslintErrors": 2,
        "prettierIssues": 5
      }
    },
    ...
  ]
}


⸻

🔄 Scoring Overview

Category	Weight	Criteria
Compilation	10	Must compile in strict mode
Linting & Formatting	10	Zero ESLint errors + Prettier passes
Testing & Coverage	10	All tests pass + coverage above threshold
Schema Validation	10	All API/schema outputs match expectations
Runtime Behavior	10	Watchers and concurrency checks pass
LLM Review (Optional)	—	Advisory only unless enabled in scoring


⸻

🧩 Extensibility
	•	Add new check categories via modular plugin interface.
	•	Supports project-specific config via aiqtest.config.json.
	•	Designed for future support of other ecosystems (Python, Java, etc).

⸻

🧪 Example Output (Console Summary)

Compilation: PASS (10/10)
Linting: FAIL (8/10) – 2 ESLint errors, 5 files need formatting
Tests: PASS (10/10) – 120 tests passed, coverage 82%
Schema: PASS (10/10)
Runtime: PASS (10/10)
LLM Review: (skipped)

Overall: 48/50 (96%) – QUALITY OK


⸻

🔐 Security & Privacy
	•	No code is ever sent to external services by default.
	•	LLM features require explicit opt-in and API key config.
	•	Runtime checks run in subprocesses with sandboxing where feasible.

⸻

📈 Roadmap Highlights
	•	Security vulnerability scanning
	•	Support for Python, Java, etc.
	•	HTML/Markdown report generation
	•	GitHub Action / CI Mode
	•	VS Code Extension

⸻

🤝 Contributing

We welcome contributions for custom checks, rule improvements, or language extensions. Future versions will include a plugin API and configuration schema.

⸻

📄 License

MIT

⸻

💬 Questions?

Open an issue or discussion on the GitHub repo or reach out via email.

Let me know if you'd like this rewritten for publishing to npm (with `bin` section, `keywords`, etc.) or as a GitHub landing page with screenshots and badges.
